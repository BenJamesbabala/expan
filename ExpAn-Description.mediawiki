= ExpAn - Statistical analysis of A/B tests in Python =

The functions in the library are standalone and can be imported and used from within any project and from the command line.

Loading of data from various sources is ''not'' in scope of this library.

= Overview =

== Assumptions used in analysis ==

* Sample-size estimation:
** Treatment does not affect variance
** Variance in treatment and control is identical
** Mean of delta is normally distributed
* Welch t-test:
** Mean of means is t-distributed (or normally distributed)
* In general:
** Sample represents underlying population
** Entities are independent

= Core Analysis Module =

Main user stories:

As a Data Scientist I want to perform all the basic analysis routines that are typical of a the analysis of an A/B Test (a.k.a. Between-Subject Randomised Control Trial) while retaining access to the raw data so I can perform very also custom analyses in order to answer the questions of stakeholders with little effort.

As an analyst from a different department, I want to be able to bring my own data, and easily be able to use this library to perform analysis: in other words, as long as data is in a format compatible with <code>expan.ExperimentData</code> (documented below), importing it into the library and then performing analyses on it should be almost trivial.

== Input Data (<code>expan.ExperimentData</code>) ==

* Data to be analysed is loaded into the <code>ExperimentData</code> class.
* Features and KPIs are stored separately (but are exposed as a single object 'metrics' by dynamically joining the two)
* An ExperimentData therefore contains:
** 2 pandas DataFrame objects (<code>kpis</code> and <code>features</code>)
** a dictionary for metadata
** a property (<code>metrics</code>) which dynamically returns another DataFrame
** a set of functions and properties to simplify access to the data somewhat
* Analysis functionality is provided on a subclass of this (<code>expan.Experiment</code>)

The tables below define the data structure of the individual parts within an ExperimentData object.

Underlined <span style="text-decoration: underline;">column names</span> refer to indices; bold is any column or row name; and square brackets indicate [an optional piece of data].

=== Metadata ===

This is a dictionary of information describing the experiment to be analysed.

{|
!width="33%"|'''key'''
!width="33%"|'''example value'''
!width="33%"|'''explanation'''
|-
|experiment
|"Generic Website Improvement"
|Name of the experiment, as known to stakeholders. Can be anything meaningful to you.
|-
|[experiment_id]
|"a9a9e987a9f99d3_2015-01-01T12:00:00.123"
|This uniquely identifies the experiment. Could be a concatenation of the experiment name and the experiment start timestamp.

|-
|sources
|["our_mysql","website_logs"]
|Names of the data sources used in the preparation of this data.<br />
|-
|baseline_variant
|“No Change”
|the variant against which all others will be measured.
|-
|[retrieval_time]
|2015-10-21H18:28CEST
|time that data was fetched from original sources... perhaps this should be a list with entry per source?<br />
|-
|[primary_KPI]
|"orders"
|Overall Evaluation Criteria
|}


=== KPIs ===

{|
!'''<span style="text-decoration: underline;">variant</span>'''
!'''<span style="text-decoration: underline;">entity</span>'''
!'''<span style="text-decoration: underline;">[time_since_treatment]</span>'''
!'''orders'''
!'''profit_contribution'''
|-
|A
|ec0231efh
|0
|1
|23.23
|-
|A
|ec0231efh
|1
|2
|250.32
|-
|B
|f387534e2
|0
|0
| - 
|-
|}


=== Features ===


{|
!'''<span style="color: rgb(0,0,0);text-decoration: underline;">variant</span>'''
!'''<span style="color: rgb(0,0,0);text-decoration: underline;">entity</span>'''
!'''treatment start time'''
!'''age'''
!'''profit_contribution_365'''
|-
|A
|ec0231efh
|2015-02-23H12:00CEST
|32
|932.92
|-
|B
|f387534e2
|2015-02-23H12:00CEST
|65
|23.44
|}


== Output Data (<code>expan.Results</code>) ==

The Results object is based on a single pandas DataFrame object. Currently it '''has-a''' DataFrame, but could in the future be implemented so that it '''is-a''' DataFrame.

Similar to the input data, Results have metadata (a dictionary) and a DataFrame.

=== Metadata ===

This is a dictionary describing the results, some of which is derived directly from the metadata of the input data, and some is additional.

{|
!'''key'''
!'''example value<br />'''
!'''explanation'''
|-
|experiment
|“Generic Website Improvement 2015-01-01”
|''see ExperimentData metadata''
|-
|experiment_id
|"a9a9e987a9f99d3_2015-01-01T12:00:00.123"
|see ExperimentData metadata
|-
|retrieval_time
|''see above''
|retrieval time of the data sources
|-
|analysis_time
|2015-10-21H18:28CEST
|Time that the analysis was performed.

''not yet implemented''
|-
|baseline_variant
|“No Change”
|Variant against which all results were computed.
|-
|primary_KPI
|"profit_contribution"
|The KPI used for OEC (Overall Evaluation Criteria).
|-
|metric_units
|<br />

{|
|orders
|orders
|-
|sample_size
|customers
|-
|net_sales
|€
|-
|profit_contribution
|€
|-
|profit_contribution_per_customer
|€
|-
|Age
|years
|}


|The underlying unit of each metric.

''not yet implemented''

Probably this can be combined with the full metric object?
|-
|cost_of_treatment
|{'A': 1, 'B': 1.5}
|Cost of treatment per variant as a dict used to offset the uplift.
|-
|expan_version
|1.0.1
|The version of expan that was used to compute the results.
|-
|[analysts]
|["joe.bloggs@zalando.de"]
|Identification of the data scientists running the analysis: probably email address is best here.

Will be a list, but is optional.
|}

==== Binning ====

The binning objects are stored as a dictionary of 'Binning' objects in the Results structure, indicating how the subgroups were created.

The bin associated with a subgroup in the results dataframe is referenced by the string label.

{|
!width="25%"|subgroup_metric
!width="25%"|binning
!width="25%"|label_format_str (going to deprecate this)
!width="25%"|label_example (not actually in results)
|-
|Age
|<pre><Binning Object Created on Age data></pre>
* bin0: 0-19
* bin1: 20-30
* bin2: 30-99
|<pre>'{lo},{hi}'</pre>
|<pre>20-30</pre>
|-
|profit_contribution_365
|<pre><Binning Object Created on profit_contribution_365 data></pre>
|<pre>'{standard}'</pre>
|<pre>[102.0,144.5)</pre>

|}

=== Result Data Frame (<code>.df</code>) ===

''yellow statistics will probably be derived: calculated on the fly by properties rather than stored in the dataframe.''

   
{|  
!  colspan="6" | <p style="text-align: center;"><em>index</em></p>
!  colspan="3" | <p style="text-align: center;"><em><u>variant columns</u></em></p>
! <p style="text-align: center;"><u><em>subgroup columns (think about this)</em></u></p>
!  rowspan="2" style="text-align: center;" | comments (not in data)
|- 
! <p><u><strong>metric</strong></u></p>
! <p><u><strong>subgroup_metric</strong></u></p>
! <p><u><strong>subgroup</strong></u></p>
! <p><u><strong>time_since_treatment</strong></u></p>
! <p><u><strong>statistic</strong></u></p>
!  colspan="1" | <u>pctile</u>
! <p><strong>“Bamboozle”</strong></p>
! <p><strong>“Spektakulatrix”</strong></p>
!  colspan="1" | "No Change"
!  colspan="1" | subgroup_bin_index
|- 
|  rowspan="18" | <p>profit_contribution</p>
|  rowspan="18" | <p>Age</p>
|  rowspan="18" | <p>20-30</p>
|  rowspan="13" | <p>0</p>
| <p>uplift</p>
|  colspan="1" | nan
| <p>3.2</p>
| <p>3.5</p>
|  colspan="1" | 0
|  rowspan="18" | 0
|  colspan="1" | the mean of the difference between variant and baseline (variant-baseline)
|- 
|  colspan="1"  | uplift_rel
|  colspan="1" | nan
|   colspan="1"  | 16%
|  colspan="1"  | 17.5%
|   colspan="1"  | 0%
|   colspan="1"  | <p>the uplift as proportion of baseline ((variant-baseline)/baseline)</p><p>NB: probably won't be in the dataframe itself because it can be derived (so prob. implement as a property of the results class)</p>
|- 
| <p>sample_size</p>
|  colspan="1" | nan
| <p>10000</p>
| <p>5000</p>
|  colspan="1" | 1000
|   | sample size of each variant
|- 
| <p>uplift_pctile</p>
|  colspan="1" | 2.5
| <p>-0.3</p>
| <p>1.2</p>
|  colspan="1" | nan
|   | percentiles of the difference between variant and baseline (so 95% confidence intervals are represented by the 2.5 and 97.5 percentiles
|- 
| <p>uplift_pctile</p>
|  colspan="1" | 97.5
| <p>7.8</p>
| <p>7.4</p>
|  colspan="1" | nan
|   | 
|- 
|  colspan="1" | uplift_pctile
|  colspan="1" | 4.3
|  colspan="1" | 0
|  colspan="1" | 0
|  colspan="1" | nan
|  | any percentile can be represented, including some special ones, like those associated with 0 uplift or uplift of exactly treatment cost.
|- 
|  colspan="1"  | prob_uplift_over_0
|   colspan="1" | nan
|   colspan="1"  | 0.043
|   colspan="1"  | 
|  colspan="1"  | nan
|    rowspan="2" | could represent the probability of uplift being over 0 explicitly like this, equivalent to having the uplift_pctile statistic with a value of 0. Discussion is <a href="https://github.bus.zalan.do/octopus/expan/issues/77">here</a> (only internal to Zalando currently, sorry)
|- 
|  class="highlight-yellow" colspan="1" data-highlight-colour="yellow" | prob_uplift_over_cost
|  class="highlight-yellow" colspan="1" data-highlight-colour="yellow" | 
|  class="highlight-yellow" colspan="1" data-highlight-colour="yellow" | 
|  class="highlight-yellow" colspan="1" data-highlight-colour="yellow" | 
|  class="highlight-yellow" colspan="1" data-highlight-colour="yellow" | 
|- 
| <p>variant_mean</p>
|  colspan="1" | nan
| <p>23.2</p>
| <p>23.5</p>
|  colspan="1" | 20
|  class="highlight-blue" data-highlight-colour="blue" | simply the mean of the variant, including baseline
|- 
|  colspan="1" | pre_treatment_diff
|  colspan="1" | nan
|  colspan="1" | 2.63
|  colspan="1" | -1.23
|  colspan="1" | 0
|  class="highlight-blue" colspan="1" data-highlight-colour="blue" | feature check result for numerical variables
|- 
|  colspan="1" | pre_treatment_diff_pctile
|  colspan="1" | 2.5
|  colspan="1" | -2.54
|  colspan="1" | -2.46
|  colspan="1" | -1.53
|  class="highlight-blue" colspan="1" data-highlight-colour="blue" | feature check result for numerical variables
|- 
|  colspan="1" | pre_treatment_diff_pctile
|  colspan="1" | 97.5
|  colspan="1" | 5.34
|  colspan="1" | 0.64
|  colspan="1" | 1.56
|  class="highlight-blue" colspan="1" data-highlight-colour="blue" | feature check result for numerical variables
|- 
|  colspan="1" | chi_square_p
|  colspan="1" | nan
|  colspan="1" | 0.63
|  colspan="1" | 0.25
|  colspan="1" | 0.93
|  class="highlight-blue" colspan="1" data-highlight-colour="blue" | feature check result for categorical variables
|- 
|  rowspan="5" | <p>10</p>
| <p>uplift</p>
|  colspan="1" | nan
| <p>5.2</p>
| 22.1
|  colspan="1" | 0
|  class="highlight-blue" data-highlight-colour="blue" | 
|- 
| <p>sample_size</p>
|  colspan="1" | nan
| <p>10000</p>
| 5000
|  colspan="1" | 1000
|  class="highlight-blue" data-highlight-colour="blue" | 
|- 
| <p>uplift_pctile</p>
|  colspan="1" | 2.5
| <p>0.9</p>
| 10.0
|  colspan="1" | nan
|  class="highlight-blue" data-highlight-colour="blue" | 
|- 
| <p>uplift_pctile</p>
|  colspan="1" | 97.5
| <p>7.8</p>
| 30.0
|  colspan="1" | nan
|  class="highlight-blue" data-highlight-colour="blue" | 
|- 
|  colspan="1" | variant_mean
|  colspan="1" | nan
|  colspan="1" | 27.2
|  colspan="1" | 44.1
|  colspan="1" | 22
|  class="highlight-blue" data-highlight-colour="blue" | 
|}

* 'time_since_treatment' is currently only included if a trend analysis was done.
* '-' is used as a sentinal for NaNs for index levels 'metric', 'subgroup_metric','subgroup' because all-nan index levels cause big problems with pandas reindexing etc.
** could think of dropping index levels if they are all nans - as time level is.
* Variants are stored in first level of columns.
** Storing baseline_variant as a piece of metadata means we do not ''need'' a column for it, and we will most likely have no use case for combining results with different baseline variants. However, we store the baseline variant in the data as an explicit column because this will allow the same structure to be used for plotting the variants directly against each other, and allows for storing the absolute (within-variant) values as well as the uplift information in the same format.

-----

= Questions =

== Open<br /> ==

== Answered ==

* [Core] Should the feature and kpi data frames be combined?
** No, we will store them separately and combine them when needed with a join. This join can be cached, but keeping separate allows efficiency especially for time-dependent analysis where features do not change.
* [Core] Should features and kpis be class objects (metric class as in Default Analyzer)?
** Attributes specific to individual metrics can be captured with dictionaries in the metadata where the metric name is the key to the dictionary (e.g. metadata['is_categorical']={'orders': False, 'gender': True}
* [Core] Should the metadata and the core data frame be combined into one unified structure?
** No. Metadata is global to the whole dataframe, it does not apply to individual elements in it. Also, it should be very easily understood and able to be manipulated: analysts should be able to store extra stuff in there as they like.
* [General] Connection to statistical monitoring module?
** That should be something dealt with in the Analysis Service

-----

= Glossary =

{|
!Name
!Definition
!Example
|-
|Metric
|Metric is the generic term covering KPI and Feature. It describes anything that can be measured on a per entity level.
|
|-
|KPI
|Key Performance Indicator. Used here to describe the data measured after the start of the treatment. It is used to identify the variables which are expected to be influenced by the treatment.
|'''profit_contribution''' accumulated after treatment start is a typical KPI for customer based experiments.
|-
|Feature
|A feature is data which is not expected to be influenced by the treatment. That includes but is not limited to all data that is known on an entity at the start of a treatment.
|Age or gender are typical features in customer based experiments. Metrics accumulated ''before'' treatment ('''profit_contribution_365''') are also features.
|}
